<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Intelligent Robotic Manipulation Lab </title> <meta name="author" content="Intelligent Robotic Manipulation Lab"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://thu-da-robotics.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">IRM</span> Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/members/">Members </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <p>The most accurate list of publications can be found on <a href="https://scholar.google.com/citations?user=6EIX-JQAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a>.</p> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yan2023multimodal-480.webp 480w,/assets/img/publication_preview/yan2023multimodal-800.webp 800w,/assets/img/publication_preview/yan2023multimodal-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/yan2023multimodal.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yan2023multimodal.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yan2024unified" class="col-sm-8"> <div class="title">A Unified Interaction Control Framework for Safe Robotic Ultrasound Scanning with Human-Intention-Aware Compliance</div> <div class="author"> <a href="https://yanseim.github.io" rel="external nofollow noopener" target="_blank">Xiangjie Yan</a>, Luo Shaqi, Yongpeng Jiang, <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, <a href="https://calaw.cc" rel="external nofollow noopener" target="_blank">Chen Chen</a>, Senqiang Zhu, Gao Huang, Shiji Song, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2302.05685" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://yanseim.github.io/iros24ultrasound/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yan2024unified</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Unified Interaction Control Framework for Safe Robotic Ultrasound Scanning with Human-Intention-Aware Compliance}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Xiangjie and Shaqi, Luo and Jiang, Yongpeng and Yu, Mingrui and Chen, Chen and Zhu, Senqiang and Huang, Gao and Song, Shiji and Li, Xiang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2302.05685}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2023generalizable-480.webp 480w,/assets/img/publication_preview/yu2023generalizable-800.webp 800w,/assets/img/publication_preview/yu2023generalizable-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/yu2023generalizable.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2023generalizable.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yu2023generalizable" class="col-sm-8"> <div class="title">Generalizable whole-body global manipulation of deformable linear objects by dual-arm robot in 3-D constrained environments</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Kangchen Lv, Changhao Wang, Yongpeng Jiang, Masayoshi Tomizuka, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>The International Journal of Robotics Research</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2310.09899" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://mingrui-yu.github.io/DLO_planning_2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2023generalizable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generalizable whole-body global manipulation of deformable linear objects by dual-arm robot in 3-D constrained environments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Lv, Kangchen and Wang, Changhao and Jiang, Yongpeng and Tomizuka, Masayoshi and Li, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The International Journal of Robotics Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2024inhand-480.webp 480w,/assets/img/publication_preview/yu2024inhand-800.webp 800w,/assets/img/publication_preview/yu2024inhand-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/yu2024inhand.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2024inhand.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yu2024inhand" class="col-sm-8"> <div class="title">In-Hand Following of Deformable Linear Objects Using Dexterous Fingers with Tactile Sensing</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Boyuan Liang, Xiang Zhang, Xinghao Zhu, <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a>, and Masayoshi Tomizuka </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2403.12676" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://mingrui-yu.github.io/DLO_following/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yu2024inhand</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Liang, Boyuan and Zhang, Xiang and Zhu, Xinghao and Li, Xiang and Tomizuka, Masayoshi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In-Hand Following of Deformable Linear Objects Using Dexterous Fingers with Tactile Sensing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/jiang2024contact-480.webp 480w,/assets/img/publication_preview/jiang2024contact-800.webp 800w,/assets/img/publication_preview/jiang2024contact-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/jiang2024contact.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="jiang2024contact.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiang2024contact" class="col-sm-8"> <div class="title">Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach</div> <div class="author"> Yongpeng Jiang, <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Xinghao Zhu, Masayoshi Tomizuka, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2402.18897" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://director-of-g.github.io/in_hand_manipulation/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiang2024contact</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Yongpeng and Yu, Mingrui and Zhu, Xinghao and Tomizuka, Masayoshi and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2402.18897}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chen2024visual-480.webp 480w,/assets/img/publication_preview/chen2024visual-800.webp 800w,/assets/img/publication_preview/chen2024visual-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/chen2024visual.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chen2024visual.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2024visual" class="col-sm-8"> <div class="title">Visual Attention Based Cognitive Human–Robot Collaboration for Pedicle Screw Placement in Robot-Assisted Orthopedic Surgery</div> <div class="author"> <a href="https://calaw.cc" rel="external nofollow noopener" target="_blank">Chen Chen</a>, Qikai Zou, Yuhang Song, Shiji Song, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2405.09359" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2024visual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual Attention Based Cognitive Human--Robot Collaboration for Pedicle Screw Placement in Robot-Assisted Orthopedic Surgery}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Chen and Zou, Qikai and Song, Yuhang and Song, Shiji and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2405.09359}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yan2024complementary-480.webp 480w,/assets/img/publication_preview/yan2024complementary-800.webp 800w,/assets/img/publication_preview/yan2024complementary-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/yan2024complementary.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yan2024complementary.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yan2024complementary" class="col-sm-8"> <div class="title">A Complementary Framework for Human–Robot Collaboration With a Mixed AR–Haptic Interface</div> <div class="author"> <a href="https://yanseim.github.io" rel="external nofollow noopener" target="_blank">Xiangjie Yan</a>, Yongpeng Jiang, <a href="https://calaw.cc" rel="external nofollow noopener" target="_blank">Chen Chen</a>, Leiliang Gong, Ming Ge, Tao Zhang, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>IEEE Transactions on Control Systems Technology</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TCST.2023.3301675" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>There is invariably a tradeoff between safety and efficiency for collaborative robots (cobots) in human–robot collaborations (HRCs). Robots that interact minimally with humans can work with high speed and accuracy but cannot adapt to new tasks or respond to unforeseen changes, whereas robots that work closely with humans can but only by becoming passive to humans, meaning that their main tasks are suspended and efficiency compromised. Accordingly, this article proposes a new complementary framework for HRC that balances the safety of humans and the efficiency of robots. In this framework, the robot carries out given tasks using a vision-based adaptive controller, and the human expert collaborates with the robot in the null space. Such a decoupling drives the robot to deal with existing issues in task space [e.g., uncalibrated camera, limited field of view (FOV)] and null space (e.g., joint limits) by itself while allowing the expert to adjust the configuration of the robot body to respond to unforeseen changes (e.g., sudden invasion, change in environment) without affecting the robot’s main task. In addition, the robot can simultaneously learn the expert’s demonstration in task space and null space beforehand with dynamic movement primitives (DMPs). Therefore, an expert’s knowledge and a robot’s capability are explored and complement each other. Human demonstration and involvement are enabled via a mixed interaction interface, i.e., augmented reality (AR) and haptic devices. The stability of the closed-loop system is rigorously proved with Lyapunov methods. Experimental results in various scenarios are presented to illustrate the performance of the proposed method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yan2024complementary</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Xiangjie and Jiang, Yongpeng and Chen, Chen and Gong, Leiliang and Ge, Ming and Zhang, Tao and Li, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Control Systems Technology}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Complementary Framework for Human–Robot Collaboration With a Mixed AR–Haptic Interface}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{32}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{112-127}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TCST.2023.3301675}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1558-0865}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/jia2024efficient-480.webp 480w,/assets/img/publication_preview/jia2024efficient-800.webp 800w,/assets/img/publication_preview/jia2024efficient-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/jia2024efficient.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="jia2024efficient.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jia2024efficient" class="col-sm-8"> <div class="title">Efficient Model Learning and Adaptive Tracking Control of Magnetic Micro-Robots for Non-Contact Manipulation</div> <div class="author"> Yongyi Jia, Shu Miao, Junjian Zhou, Niandong Jiao, Lianqing Liu, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2024 IEEE International Conference on Robotics and Automation (ICRA)</em>, Jan 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.14414" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Magnetic microrobots can be navigated by an external magnetic field to autonomously move within living organisms with complex and unstructured environments. Potential applications include drug delivery, diagnostics, and therapeutic interventions. Existing techniques commonly impart magnetic properties to the target object,or drive the robot to contact and then manipulate the object, both probably inducing physical damage. This paper considers a non-contact formulation, where the robot spins to generate a repulsive field to push the object without physical contact. Under such a formulation, the main challenge is that the motion model between the input of the magnetic field and the output velocity of the target object is commonly unknown and difficult to analyze. To deal with it, this paper proposes a data-driven-based solution. A neural network is constructed to efficiently estimate the motion model. Then, an approximate model-based optimal control scheme is developed to push the object to track a time-varying trajectory, maintaining the non-contact with distance constraints. Furthermore, a straightforward planner is introduced to assess the adaptability of non-contact manipulation in a cluttered unstructured environment. Experimental results are presented to show the tracking and navigation performance of the proposed scheme.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jia2024efficient</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Model Learning and Adaptive Tracking Control of Magnetic Micro-Robots for Non-Contact Manipulation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jia, Yongyi and Miao, Shu and Zhou, Junjian and Jiao, Niandong and Liu, Lianqing and Li, Xiang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chensafe23-480.webp 480w,/assets/img/publication_preview/chensafe23-800.webp 800w,/assets/img/publication_preview/chensafe23-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/chensafe23.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chensafe23.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2023safe" class="col-sm-8"> <div class="title">Safe and Individualized Motion Planning for Upper-limb Exoskeleton Robots Using Human Demonstration and Interactive Learning</div> <div class="author"> Yu Chen , Gong Chen, Jing Ye, Xiangjun Qiu, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2024 International Conference on Robotics and Automation (ICRA)</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2309.08178" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2023safe</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Safe and Individualized Motion Planning for Upper-limb Exoskeleton Robots Using Human Demonstration and Interactive Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Yu and Chen, Gong and Ye, Jing and Qiu, Xiangjun and Li, Xiang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2309.08178}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chen23HIL-480.webp 480w,/assets/img/publication_preview/chen23HIL-800.webp 800w,/assets/img/publication_preview/chen23HIL-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/chen23HIL.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chen23HIL.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2023learning" class="col-sm-8"> <div class="title">Learning to Assist Different Wearers in Multitasks: Efficient and Individualized Human-In-the-Loop Adaption Framework for Exoskeleton Robots</div> <div class="author"> Yu Chen, Shu Miao , Gong Chen, Jing Ye, Chenglong Fu, Bin Liang, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2309.14720" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">chen2023learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Assist Different Wearers in Multitasks: Efficient and Individualized Human-In-the-Loop Adaption Framework for Exoskeleton Robots}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Yu and Miao, Shu and Chen, Gong and Ye, Jing and Fu, Chenglong and Liang, Bin and Li, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Robotics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2309.14720}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/jiang2023contact-480.webp 480w,/assets/img/publication_preview/jiang2023contact-800.webp 800w,/assets/img/publication_preview/jiang2023contact-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/jiang2023contact.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="jiang2023contact.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiang2023contact" class="col-sm-8"> <div class="title">Contact-Aware Non-prehensile Robotic Manipulation for Object Retrieval in Cluttered Environments</div> <div class="author"> Yongpeng Jiang, Yongyi Jia, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/IROS55552.2023.10341476" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2303.03635" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://director-of-g.github.io/push_in_clutter/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiang2023contact</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Yongpeng and Jia, Yongyi and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contact-Aware Non-prehensile Robotic Manipulation for Object Retrieval in Cluttered Environments}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10604-10611}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS55552.2023.10341476}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/shu23two-480.webp 480w,/assets/img/publication_preview/shu23two-800.webp 800w,/assets/img/publication_preview/shu23two-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/shu23two.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="shu23two.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shu2023two" class="col-sm-8"> <div class="title">Two-Stage Trajectory-Tracking Control of Cable-Driven Upper-Limb Exoskeleton Robots with Series Elastic Actuators: A Simple, Accurate, and Force-Sensorless Method</div> <div class="author"> Yana Shu , Yu Chen, Xuan Zhang, Shisheng Zhang , Gong Chen, Jing Ye, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/IROS55552.2023.10342056" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2023global-480.webp 480w,/assets/img/publication_preview/yu2023global-800.webp 800w,/assets/img/publication_preview/yu2023global-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/yu2023global.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2023global.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yu2023global" class="col-sm-8"> <div class="title">Global Model Learning for Large Deformation Control of Elastic Deformable Linear Objects: An Efficient and Adaptive Approach</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Kangchen Lv, Hanzhong Zhong, Shiji Song, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>IEEE Transactions on Robotics</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/TRO.2022.3200546" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2205.04004" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Mingrui-Yu/shape_control_DLO_2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mingrui-yu.github.io/shape_control_DLO_2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2023global</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Lv, Kangchen and Zhong, Hanzhong and Song, Shiji and Li, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Robotics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Global Model Learning for Large Deformation Control of Elastic Deformable Linear Objects: An Efficient and Adaptive Approach}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{417-436}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TRO.2022.3200546}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2023acoarse-480.webp 480w,/assets/img/publication_preview/yu2023acoarse-800.webp 800w,/assets/img/publication_preview/yu2023acoarse-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/yu2023acoarse.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2023acoarse.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yu2023acoarse" class="col-sm-8"> <div class="title">A Coarse-to-Fine Framework for Dual-Arm Manipulation of Deformable Linear Objects with Whole-Body Obstacle Avoidance</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Kangchen Lv, Changhao Wang, Masayoshi Tomizuka, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA48891.2023.10160264" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2209.11145" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://mingrui-yu.github.io/DLO_planning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yu2023acoarse</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Lv, Kangchen and Wang, Changhao and Tomizuka, Masayoshi and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Coarse-to-Fine Framework for Dual-Arm Manipulation of Deformable Linear Objects with Whole-Body Obstacle Avoidance}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10153-10159}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48891.2023.10160264}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lv2023learning-480.webp 480w,/assets/img/publication_preview/lv2023learning-800.webp 800w,/assets/img/publication_preview/lv2023learning-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/lv2023learning.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lv2023learning.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lv2023learning" class="col-sm-8"> <div class="title">Learning to Estimate 3-D States of Deformable Linear Objects from Single-Frame Occluded Point Clouds</div> <div class="author"> Kangchen Lv, <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Yifan Pu, Xin Jiang, Gao Huang, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA48891.2023.10160784" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2210.01433" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lv2023learning</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lv, Kangchen and Yu, Mingrui and Pu, Yifan and Jiang, Xin and Huang, Gao and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Estimate 3-D States of Deformable Linear Objects from Single-Frame Occluded Point Clouds}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7119-7125}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Point cloud compression;Learning systems;Geometry;Solid modeling;Automation;Shape;Wires}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48891.2023.10160784}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/zhan23multi-480.webp 480w,/assets/img/publication_preview/zhan23multi-800.webp 800w,/assets/img/publication_preview/zhan23multi-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/zhan23multi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="zhan23multi.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023multi" class="col-sm-8"> <div class="title">Multi-Modal Learning and Relaxation of Physical Conflict for an Exoskeleton Robot with Proprioceptive Perception</div> <div class="author"> Xuan Zhang, Yana Shu , Yu Chen , Gong Chen, Jing Ye , Xiu Li, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA48891.2023.10161255" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/jia2022hierarchical-480.webp 480w,/assets/img/publication_preview/jia2022hierarchical-800.webp 800w,/assets/img/publication_preview/jia2022hierarchical-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/jia2022hierarchical.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="jia2022hierarchical.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jia2022hierarchical" class="col-sm-8"> <div class="title">Hierarchical Learning and Control for In-Hand Micromanipulation Using Multiple Laser-Driven Micro-Tools</div> <div class="author"> Yongyi Jia , Yu Chen, Hao Liu , Xiu Li, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Laser-driven micro-tools are formulated by treating highly-focused laser beams as actuators, to control the tool’s motion to contact then manipulate a micro object, which allows it to manipulate opaque micro objects, or large cells without causing photodamage. However, most existing laser-driven tools are limited to relatively simple tasks, such as moving and caging, and cannot carry out in-hand dexterous tasks. This is mainly because in-hand manipulation involves continuously coordinating multiple laser beams, micro-tools, and the object itself, which has high degrees of freedom (DoF) and poses up challenge for planner and controller design. This paper presents a new hierarchical formulation for the grasping and manipulation of micro objects using multiple laser-driven micro-tools. In hardware, multiple laser-driven tools are assembled to act as a robotic hand to carry out in-hand tasks (e.g., rotating); in software, a hierarchical scheme is developed to shrunken the action space and coordinate the motion of multiple tools, subject to both the parametric uncertainty in the tool and the unknown dynamic model of the object. Such a formulation provides potential for achieving robotic in-hand manipulation at a micro scale. The performance of the proposed system is validated in simulation studies under different scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jia2022hierarchical</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchical Learning and Control for In-Hand Micromanipulation Using Multiple Laser-Driven Micro-Tools}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jia, Yongyi and Chen, Yu and Liu, Hao and Li, Xiu and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1047--1054}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yan2022adaptive-480.webp 480w,/assets/img/publication_preview/yan2022adaptive-800.webp 800w,/assets/img/publication_preview/yan2022adaptive-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/yan2022adaptive.jpeg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yan2022adaptive.jpeg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yan2022adaptive" class="col-sm-8"> <div class="title">Adaptive Vision-Based Control of Redundant Robots with Null-Space Interaction for Human-Robot Collaboration</div> <div class="author"> <a href="https://yanseim.github.io" rel="external nofollow noopener" target="_blank">Xiangjie Yan</a>, <a href="https://calaw.cc" rel="external nofollow noopener" target="_blank">Chen Chen</a>, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2022 International Conference on Robotics and Automation (ICRA)</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICRA46639.2022.9812218" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/yanseim/Vision-Based-Control" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Human-robot collaboration aims to extend human ability through cooperation with robots. This technology is currently helping people with physical disabilities, has transformed the manufacturing process of companies, improved surgical performance, and will likely revolutionize the daily lives of everyone in the future. Being able to enhance the performance of both sides, such that human-robot collaboration outperforms a single robot/human, remains an open issue. For safer and more effective collaboration, a new control scheme has been proposed for redundant robots in this paper, consisting of an adaptive vision-based control term in task space and an interactive control term in null space. Such a formulation allows the robot to autonomously carry out tasks in an unknown environment without prior calibration while also interacting with humans to deal with unforeseen changes (e.g., potential collision, temporary needs) under the redundant configuration. The decoupling between task space and null space helps to explore the collaboration safely and effectively without affecting the main task of the robot end-effector. The stability of the closed-loop system has been rigorously proved with Lyapunov methods, and both the convergence of the position error in task space and that of the damping model in null space are guaranteed. The experimental results of a robot manipulator guided with the technology of augmented reality (AR) are presented to illustrate the performance of the control scheme.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yan2022adaptive</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yan, Xiangjie and Chen, Chen and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adaptive Vision-Based Control of Redundant Robots with Null-Space Interaction for Human-Robot Collaboration}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2803-2809}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA46639.2022.9812218}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2022shape-480.webp 480w,/assets/img/publication_preview/yu2022shape-800.webp 800w,/assets/img/publication_preview/yu2022shape-1400.webp 1400w," type="image/webp" sizes="600px"></source> <img src="/assets/img/publication_preview/yu2022shape.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2022shape.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yu2022shape" class="col-sm-8"> <div class="title">Shape Control of Deformable Linear Objects with Offline and Online Learning of Local Linear Deformation Models</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Hanzhong Zhong, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2022 International Conference on Robotics and Automation (ICRA)</em>, May 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA46639.2022.9812244" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2109.11091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Mingrui-Yu/shape_control_DLO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mingrui-yu.github.io/shape_control_DLO/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yu2022shape</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Zhong, Hanzhong and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Shape Control of Deformable Linear Objects with Offline and Online Learning of Local Linear Deformation Models}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1337-1343}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA46639.2022.9812244}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Intelligent Robotic Manipulation Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>