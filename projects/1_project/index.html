<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Manipulation of Deformable Objects | Intelligent Robotic Manipulation Lab </title> <meta name="author" content="Intelligent Robotic Manipulation Lab"> <meta name="description" content="such as cables and ropes"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://thu-da-robotics.github.io/projects/1_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">IRM</span> Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/members/">Members </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Manipulation of Deformable Objects</h1> <p class="post-description">such as cables and ropes</p> </header> <article> <h2 id="overview">Overview</h2> <p>Deformable linear objects (DLOs), such as cables, wires, and ropes, are prevalent in various everyday manipulation scenarios. Distinguished from rigid objects, deformable objects exhibit strong deformation natures, complex models, and significant individual differences. These present new challenges when using existing robotic manipulation methods and necessitate manual intervention in practical applications. We focus on the robotic dexterous manipulation of DLOs. By addressing scientific problems such as constrained global modeling and real-time dexterous policy solving, we aim to achieve autonomous and intelligent manipulation of DLOs by dual-arm robots.</p> <p>We establish a complete manipulation framework involving DLO modeling, whole-body moving and shaping, and precise terminal manipulation. The main research includes large deformation modeling and efficient model learning, whole-body shape control and global planning in constrained environments, and precise terminal manipulation based on visual and tactile sensing. We seek to comprehensively address open challenges arising from the strong deformation and high variability of DLOs, such as model uncertainty, high dimension, multiple constraints, under-actuation, visual occlusion, and generalization difficulties.</p> <h2 id="in-hand-following-of-deformable-linear-objects-using-dexterous-fingers-with-tactile-sensing">In-Hand Following of Deformable Linear Objects Using Dexterous Fingers With Tactile Sensing</h2> <p><a class="citation" href="#yu2024inhand">(Yu et al., 2024)</a> [<a href="https://mingrui-yu.github.io/DLO_following/" rel="external nofollow noopener" target="_blank">Website</a>]</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="https://mingrui-yu.github.io/files/24_iros_dlo_following.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>Most research on deformable linear object (DLO) manipulation assumes rigid grasping. However, beyond rigid grasping and re-grasping, in-hand following is also an essential skill that humans use to dexterously manipulate DLOs. In this work, inspired by how humans use fingers to follow DLOs, we explore the usage of a generic dexterous hand with tactile sensing to imitate human skills and achieve robust in-hand DLO following. To enable the hardware system to function in the real world, we develop a framework that includes Cartesian-space arm-hand control, tactile-based in-hand 3-D DLO pose estimation, and task-specific motion design.</p> <h2 id="generalizable-global-manipulation-of-deformable-linear-objects-in-constrained-environments">Generalizable Global Manipulation of Deformable Linear Objects in Constrained Environments</h2> <p><a class="citation" href="#yu2023generalizable">(Yu et al., 2024)</a> [<a href="https://mingrui-yu.github.io/DLO_planning_2/" rel="external nofollow noopener" target="_blank">Website</a>]</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="https://mingrui-yu.github.io/files/23_DLO_planning_journal.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>This article focuses on the global moving and shaping of DLOs in constrained environments by dual-arm robots. The main objectives are 1) to efficiently and accurately accomplish this task, and 2) to achieve generalizable and robust manipulation of various DLOs. To this end, we propose a complementary framework with whole-body planning and control using appropriate DLO model representations. Experiments demonstrate that our framework can accomplish considerably more complicated tasks than existing works. It achieves a 100% planning success rate among thousands of trials with an average time cost of less than 15 second, and a 100% manipulation success rate among 135 real-world tests on five different DLOs.</p> <h2 id="dual-arm-manipulation-of-deformable-linear-objects-with-whole-body-obstacle-avoidance">Dual-Arm Manipulation of Deformable Linear Objects with Whole-Body Obstacle Avoidance</h2> <p><a class="citation" href="#yu2023acoarse">(Yu et al., 2023)</a> [<a href="https://mingrui-yu.github.io/DLO_planning/" rel="external nofollow noopener" target="_blank">Website</a>]</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://mingrui-yu.github.io/files/23_icra_planning-480.webp 480w,https://mingrui-yu.github.io/files/23_icra_planning-800.webp 800w,https://mingrui-yu.github.io/files/23_icra_planning-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://mingrui-yu.github.io/files/23_icra_planning.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Manipulating deformable linear objects (DLOs) to achieve desired shapes in constrained environments with obstacles is a meaningful but challenging task. We propose a coarse-to-fine framework to combine global planning and local control for dual-arm manipulation of DLOs, capable of precisely achieving desired configurations and avoiding potential collisions between the DLO, robot, and obstacles. Both simulations and real-world experiments demonstrate that our framework can robustly achieve desired DLO configurations in constrained environments with imprecise DLO models, which may not be reliably achieved by only planning or control.</p> <h2 id="learning-to-estimate-3-d-states-of-deformable-linear-objects-from-occluded-single-frame-point-clouds">Learning to Estimate 3-D States of Deformable Linear Objects from Occluded Single-Frame Point Clouds</h2> <p><a class="citation" href="#lv2023learning">(Lv et al., 2023)</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://mingrui-yu.github.io/files/23_icra_perception-480.webp 480w,https://mingrui-yu.github.io/files/23_icra_perception-800.webp 800w,https://mingrui-yu.github.io/files/23_icra_perception-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://mingrui-yu.github.io/files/23_icra_perception.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We focus on learning to robustly estimate the states of DLOs from single-frame point clouds in the presence of occlusions using a data-driven method. Simulation and real-world experimental results demonstrate that our method can generate globally smooth and locally precise DLO state estimation results even with heavily occluded point clouds, which can be directly applied to real-world robotic manipulation of DLOs in 3-D space.</p> <h2 id="global-model-learning-for-large-deformation-control-of-deformable-linear-objects">Global Model Learning for Large Deformation Control of Deformable Linear Objects</h2> <p><a class="citation" href="#yu2023global">(Yu et al., 2023)</a> <a class="citation" href="#yu2022shape">(Yu et al., 2022)</a> [<a href="https://mingrui-yu.github.io/shape_control_DLO_2/" rel="external nofollow noopener" target="_blank">T-RO Website</a>] [<a href="https://mingrui-yu.github.io/shape_control_DLO/" rel="external nofollow noopener" target="_blank">ICRA Website</a>]</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://mingrui-yu.github.io/files/22_TRO-480.webp 480w,https://mingrui-yu.github.io/files/22_TRO-800.webp 800w,https://mingrui-yu.github.io/files/22_TRO-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="https://mingrui-yu.github.io/files/22_TRO.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We propose a coupled offline and online data-driven method for efficiently learning a global deformation model, allowing for both accurate modeling through offline learning and further updating for new DLOs via online adaptation. We also propose a convex-optimization-based controller and analyze the system’s stability using the Lyapunov method. Detailed simulations and real-world experiments demonstrate that our method can efficiently and precisely estimate the deformation model, and achieve large deformation control of untrained DLOs in 2D and 3D dual-arm manipulation tasks better than the existing methods. learning.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2024inhand-480.webp 480w,/assets/img/publication_preview/yu2024inhand-800.webp 800w,/assets/img/publication_preview/yu2024inhand-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/yu2024inhand.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2024inhand.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yu2024inhand" class="col-sm-8"> <div class="title">In-Hand Following of Deformable Linear Objects Using Dexterous Fingers with Tactile Sensing</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Boyuan Liang, Xiang Zhang, Xinghao Zhu, <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a>, and Masayoshi Tomizuka </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2403.12676" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://mingrui-yu.github.io/DLO_following/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yu2024inhand</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Liang, Boyuan and Zhang, Xiang and Zhu, Xinghao and Li, Xiang and Tomizuka, Masayoshi}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In-Hand Following of Deformable Linear Objects Using Dexterous Fingers with Tactile Sensing}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2023generalizable-480.webp 480w,/assets/img/publication_preview/yu2023generalizable-800.webp 800w,/assets/img/publication_preview/yu2023generalizable-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/yu2023generalizable.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2023generalizable.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yu2023generalizable" class="col-sm-8"> <div class="title">Generalizable whole-body global manipulation of deformable linear objects by dual-arm robot in 3-D constrained environments</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Kangchen Lv, Changhao Wang, Yongpeng Jiang, Masayoshi Tomizuka, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>The International Journal of Robotics Research</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2310.09899" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://mingrui-yu.github.io/DLO_planning_2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2023generalizable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generalizable whole-body global manipulation of deformable linear objects by dual-arm robot in 3-D constrained environments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Lv, Kangchen and Wang, Changhao and Jiang, Yongpeng and Tomizuka, Masayoshi and Li, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The International Journal of Robotics Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2023acoarse-480.webp 480w,/assets/img/publication_preview/yu2023acoarse-800.webp 800w,/assets/img/publication_preview/yu2023acoarse-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/yu2023acoarse.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2023acoarse.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yu2023acoarse" class="col-sm-8"> <div class="title">A Coarse-to-Fine Framework for Dual-Arm Manipulation of Deformable Linear Objects with Whole-Body Obstacle Avoidance</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Kangchen Lv, Changhao Wang, Masayoshi Tomizuka, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA48891.2023.10160264" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2209.11145" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://mingrui-yu.github.io/DLO_planning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yu2023acoarse</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Lv, Kangchen and Wang, Changhao and Tomizuka, Masayoshi and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Coarse-to-Fine Framework for Dual-Arm Manipulation of Deformable Linear Objects with Whole-Body Obstacle Avoidance}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10153-10159}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48891.2023.10160264}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lv2023learning-480.webp 480w,/assets/img/publication_preview/lv2023learning-800.webp 800w,/assets/img/publication_preview/lv2023learning-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/lv2023learning.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lv2023learning.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="lv2023learning" class="col-sm-8"> <div class="title">Learning to Estimate 3-D States of Deformable Linear Objects from Single-Frame Occluded Point Clouds</div> <div class="author"> Kangchen Lv, <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Yifan Pu, Xin Jiang, Gao Huang, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA48891.2023.10160784" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2210.01433" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lv2023learning</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lv, Kangchen and Yu, Mingrui and Pu, Yifan and Jiang, Xin and Huang, Gao and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Estimate 3-D States of Deformable Linear Objects from Single-Frame Occluded Point Clouds}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7119-7125}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Point cloud compression;Learning systems;Geometry;Solid modeling;Automation;Shape;Wires}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA48891.2023.10160784}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2023global-480.webp 480w,/assets/img/publication_preview/yu2023global-800.webp 800w,/assets/img/publication_preview/yu2023global-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/yu2023global.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2023global.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yu2023global" class="col-sm-8"> <div class="title">Global Model Learning for Large Deformation Control of Elastic Deformable Linear Objects: An Efficient and Adaptive Approach</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Kangchen Lv, Hanzhong Zhong, Shiji Song, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>IEEE Transactions on Robotics</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/TRO.2022.3200546" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2205.04004" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Mingrui-Yu/shape_control_DLO_2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mingrui-yu.github.io/shape_control_DLO_2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2023global</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Lv, Kangchen and Zhong, Hanzhong and Song, Shiji and Li, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Robotics}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Global Model Learning for Large Deformation Control of Elastic Deformable Linear Objects: An Efficient and Adaptive Approach}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{39}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{417-436}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TRO.2022.3200546}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/yu2022shape-480.webp 480w,/assets/img/publication_preview/yu2022shape-800.webp 800w,/assets/img/publication_preview/yu2022shape-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/yu2022shape.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="yu2022shape.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="yu2022shape" class="col-sm-8"> <div class="title">Shape Control of Deformable Linear Objects with Offline and Online Learning of Local Linear Deformation Models</div> <div class="author"> <a href="https://mingrui-yu.github.io" rel="external nofollow noopener" target="_blank">Mingrui Yu</a>, Hanzhong Zhong, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2022 International Conference on Robotics and Automation (ICRA)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA46639.2022.9812244" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2109.11091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Mingrui-Yu/shape_control_DLO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://mingrui-yu.github.io/shape_control_DLO/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yu2022shape</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yu, Mingrui and Zhong, Hanzhong and Li, Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Shape Control of Deformable Linear Objects with Offline and Online Learning of Local Linear Deformation Models}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1337-1343}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICRA46639.2022.9812244}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Intelligent Robotic Manipulation Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>