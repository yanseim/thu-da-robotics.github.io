<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Exoskeleton Robots | Intelligent Robotic Manipulation Lab </title> <meta name="author" content="Intelligent Robotic Manipulation Lab"> <meta name="description" content="rehabilitation and human augmentation"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://thu-da-robotics.github.io/projects/5_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">IRM</span> Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/members/">Members </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Exoskeleton Robots</h1> <p class="post-description">rehabilitation and human augmentation</p> </header> <article> <h2 id="overview">Overview</h2> <p>Exoskeleton robots are a type of wearable robot primarily designed for two main functions: rehabilitation training and enhancing human motor functions. Depending on the assisted body part, they can be classified into lower limb exoskeletons and upper limb exoskeletons. We focus on the interactive control and trajectory planning of exoskeleton robots. By integrating compliant actuators and generative models, we aim to develop safe, reliable lower limb and upper limb exoskeletons that can provide personalized assistance.</p> <p>We have developed a unified interactive controller and a safety-based trajectory planning framework for lower limb exoskeletons with series elastic actuators (SEA) and upper limb exoskeletons with cable-driven SEA.</p> <h2 id="multi-modal-anomaly-detection-with-proprioceptive-perception">Multi-Modal Anomaly Detection with Proprioceptive Perception</h2> <p><a class="citation" href="#zhang2023multi">(Zhang et al., 2023)</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="https://stan-32.github.io/files/zx2023.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>During the closely-coupled interaction, a mismatch between the wearer and the robot may result in physical conflict, which could affect assistance efficiency or even compromise safety. Therefore, such conflicts should be accurately detected and then properly relaxed by adjusting the robot’s action. This work proposes a new learning scheme to detect physical conflicts between humans and robots. The constructed learning network receives multi-modal information from proprioceptive sensors and then outputs the anomaly score to specify the physical conflict, which score is further used to continuously adjust the robot impedance to ensure a safe and efficient interaction.</p> <h2 id="individualized-human-in-the-loop-adaption-framework-for-lower-limb-exoskeleton">Individualized Human-In-the-Loop Adaption Framework for Lower-Limb Exoskeleton</h2> <p><a class="citation" href="#chen2023learning">(Chen et al., 2024)</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="https://stan-32.github.io/files/chen2023indi.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>The generalizability of robots across different wearers in multiple tasks is important to ensure that the robot can provide correct and effective assistance in actual implementation. However, most lower-limb exoskeleton robots exhibit only limited generalizability. Therefore, we propose a human-in-the-loop learning and adaptation framework for exoskeleton robots to improve their performance in various tasks and for different wearers. To suit different wearers, an individualized walking trajectory is generated online using dynamic movement primitives and Bayes optimization. To accommodate various tasks, a task translator is constructed using a neural network to generalize a trajectory to more complex scenarios.</p> <h2 id="interactive-control-for-cable-driven-upper-limb-exoskeleton-robots-with-series-elastic-actuators">Interactive Control for Cable-Driven Upper-Limb Exoskeleton Robots with Series Elastic Actuators</h2> <p><a class="citation" href="#shu2023two">(Shu et al., 2023)</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="https://stan-32.github.io/files/sya2023.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>We propose a new trajectory-tracking control scheme for cable-driven upper-limb exoskeleton robots with series elastic actuators. The control objectives are achieved in two stages: Stage I is to approximate then compensate for unmodelled disturbances with iterative learning techniques; Stage II is to employ a suboptimal model predictive controller to drive the robot to track the desired trajectory. While controlling such a robot is not trivial, the proposed control scheme exhibits the advantages of force-sensorlessness, high accuracy, and low complexity compared with other methods in the real-world experiments.</p> <h2 id="safe-and-individualized-motion-planning-for-upper-limb-exoskeleton">Safe and Individualized Motion Planning for Upper-limb Exoskeleton</h2> <p><a class="citation" href="#chen2023safe">(Chen et al., 2024)</a></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="https://stan-32.github.io/files/chen2023safe.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <p>We proposes a new motion planning scheme for upper-limb exoskeleton robots, which drives the robot to provide customized, safe, and individualized assistance using both human demonstration and interactive learning. Specifically, the robot first learns from a group of healthy subjects to generate a reference motion trajectory via probabilistic movement primitives (ProMP). It then learns from the patient during the training process to further shape the trajectory inside a moving safe region. The interactive data is fed back into the ProMP iteratively to enhance the individualized features for as long as the training process continues.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chen23HIL-480.webp 480w,/assets/img/publication_preview/chen23HIL-800.webp 800w,/assets/img/publication_preview/chen23HIL-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/chen23HIL.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chen23HIL.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chen2023learning" class="col-sm-8"> <div class="title">Learning to Assist Different Wearers in Multitasks: Efficient and Individualized Human-In-the-Loop Adaption Framework for Exoskeleton Robots</div> <div class="author"> Yu Chen, Shu Miao , Gong Chen, Jing Ye, Chenglong Fu, Bin Liang, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2309.14720" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">chen2023learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Assist Different Wearers in Multitasks: Efficient and Individualized Human-In-the-Loop Adaption Framework for Exoskeleton Robots}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Yu and Miao, Shu and Chen, Gong and Ye, Jing and Fu, Chenglong and Liang, Bin and Li, Xiang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Robotics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2309.14720}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chensafe23-480.webp 480w,/assets/img/publication_preview/chensafe23-800.webp 800w,/assets/img/publication_preview/chensafe23-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/chensafe23.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="chensafe23.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chen2023safe" class="col-sm-8"> <div class="title">Safe and Individualized Motion Planning for Upper-limb Exoskeleton Robots Using Human Demonstration and Interactive Learning</div> <div class="author"> Yu Chen , Gong Chen, Jing Ye, Xiangjun Qiu, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2024 International Conference on Robotics and Automation (ICRA)</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2309.08178" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen2023safe</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Safe and Individualized Motion Planning for Upper-limb Exoskeleton Robots Using Human Demonstration and Interactive Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Yu and Chen, Gong and Ye, Jing and Qiu, Xiangjun and Li, Xiang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2309.08178}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.RO}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/zhan23multi-480.webp 480w,/assets/img/publication_preview/zhan23multi-800.webp 800w,/assets/img/publication_preview/zhan23multi-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/zhan23multi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="zhan23multi.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="zhang2023multi" class="col-sm-8"> <div class="title">Multi-Modal Learning and Relaxation of Physical Conflict for an Exoskeleton Robot with Proprioceptive Perception</div> <div class="author"> Xuan Zhang, Yana Shu , Yu Chen , Gong Chen, Jing Ye , Xiu Li, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ICRA48891.2023.10161255" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/shu23two-480.webp 480w,/assets/img/publication_preview/shu23two-800.webp 800w,/assets/img/publication_preview/shu23two-1400.webp 1400w," type="image/webp" sizes="600px"> <img src="/assets/img/publication_preview/shu23two.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="shu23two.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="shu2023two" class="col-sm-8"> <div class="title">Two-Stage Trajectory-Tracking Control of Cable-Driven Upper-Limb Exoskeleton Robots with Series Elastic Actuators: A Simple, Accurate, and Force-Sensorless Method</div> <div class="author"> Yana Shu , Yu Chen, Xuan Zhang, Shisheng Zhang , Gong Chen, Jing Ye, and <a href="https://sites.google.com/view/homepageoflixiang/home" rel="external nofollow noopener" target="_blank">Xiang Li</a> </div> <div class="periodical"> <em>In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/IROS55552.2023.10342056" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Intelligent Robotic Manipulation Lab. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>